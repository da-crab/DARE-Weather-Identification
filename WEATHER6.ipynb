{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12623 images belonging to 5 classes.\n",
      "Found 2705 images belonging to 5 classes.\n",
      "Found 2710 images belonging to 5 classes.\n",
      "\n",
      "######################################################################\n",
      "### PROCESSING RUN 1/10\n",
      "######################################################################\n",
      "\n",
      "============================================================\n",
      "=== Training Custom_CNN for Run 1 ===\n",
      "============================================================\n",
      "Loading existing model from: saved_models\\Custom_CNN.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Boome\\anaconda3\\envs\\myTF\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 890ms/step - acc: 0.8142 - loss: 0.4893 - val_acc: 0.7135 - val_loss: 0.7942\n",
      "Epoch 2/50\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 883ms/step - acc: 0.8352 - loss: 0.4272 - val_acc: 0.7287 - val_loss: 0.8053\n",
      "Epoch 3/50\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 844ms/step - acc: 0.8703 - loss: 0.3487 - val_acc: 0.7213 - val_loss: 0.8383\n",
      "Epoch 4/50\n",
      "\u001b[1m78/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m16s\u001b[0m 792ms/step - acc: 0.9070 - loss: 0.2693"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import InceptionV3, VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- 1. Global Configuration ---\n",
    "# Data and Model Paths\n",
    "source_dirs = {\n",
    "    \"sunny\": \"./data/sunny\", \"cloudy\": \"./data/cloudy\",\n",
    "    \"rainy\": \"./data/rainy\", \"snowy\": \"./data/snowy\", \"foggy\": \"./data/foggy\",\n",
    "}\n",
    "base_train_dir = Path(\"./data/weather_train\")\n",
    "base_valid_dir = Path(\"./data/weather_validation\")\n",
    "base_test_dir = Path(\"./data/weather_test\")\n",
    "SAVED_MODELS_DIR = Path(\"./saved_models\")\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "\n",
    "# Data and Training Hyperparameters\n",
    "train_pct, validation_pct, test_pct = 0.70, 0.15, 0.15\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 128\n",
    "NUM_TRAINING_RUNS = 10\n",
    "\n",
    "# --- 2. Setup Functions ---\n",
    "def setup_data_directories():\n",
    "    \"\"\"(Onetime) Cleans and creates the directory structure for train/val/test splits.\"\"\"\n",
    "    print(\"--- Setting up data directories ---\")\n",
    "    for base_dir in [base_train_dir, base_valid_dir, base_test_dir]:\n",
    "        if base_dir.exists(): shutil.rmtree(base_dir)\n",
    "        for weather_type in source_dirs.keys():\n",
    "            os.makedirs(base_dir / weather_type, exist_ok=True)\n",
    "    for weather_type, source_path_str in source_dirs.items():\n",
    "        source_path = Path(source_path_str)\n",
    "        if not source_path.exists(): continue\n",
    "        all_images = [f for f in os.listdir(source_path) if os.path.isfile(source_path / f)]\n",
    "        random.shuffle(all_images)\n",
    "        total_images = len(all_images)\n",
    "        train_amount = int(total_images * train_pct)\n",
    "        validation_amount = int(total_images * validation_pct)\n",
    "        train_split = all_images[:train_amount]\n",
    "        valid_split = all_images[train_amount : train_amount + validation_amount]\n",
    "        test_split = all_images[train_amount + validation_amount :]\n",
    "        for image in train_split: shutil.copyfile(source_path / image, base_train_dir / weather_type / image)\n",
    "        for image in valid_split: shutil.copyfile(source_path / image, base_valid_dir / weather_type / image)\n",
    "        for image in test_split: shutil.copyfile(source_path / image, base_test_dir / weather_type / image)\n",
    "    print(\"Data preparation complete.\\n\")\n",
    "\n",
    "def create_data_generators():\n",
    "    \"\"\"Creates and returns train, validation, and test data generators.\"\"\"\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    train_generator = train_datagen.flow_from_directory(directory=base_train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\", seed=63)\n",
    "    valid_generator = valid_datagen.flow_from_directory(directory=base_valid_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\", seed=63)\n",
    "    test_generator = test_datagen.flow_from_directory(directory=base_test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=False, seed=63)\n",
    "    return train_generator, valid_generator, test_generator\n",
    "\n",
    "# --- 3. Model Building Functions ---\n",
    "def create_custom_cnn(input_shape, num_classes):\n",
    "    \"\"\"Builds the custom CNN from scratch.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"), layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\"), layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\"), layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(), layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_transfer_model(base_model_fn, input_shape, num_classes):\n",
    "    \"\"\"Builds a transfer learning model with a given base.\"\"\"\n",
    "    conv_base = base_model_fn(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    conv_base.trainable = False\n",
    "    model = models.Sequential([\n",
    "        conv_base,\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- 4. Plotting and Evaluation Functions ---\n",
    "def save_history_plot(history, title, save_path):\n",
    "    \"\"\"Plots training/validation accuracy and loss, then saves the figure.\"\"\"\n",
    "    acc, val_acc = history.history[\"acc\"], history.history[\"val_acc\"]\n",
    "    loss, val_loss = history.history[\"loss\"], history.history[\"val_loss\"]\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    ax1.plot(epochs, acc, \"bo\", label=\"Training acc\"); ax1.plot(epochs, val_acc, \"b\", label=\"Validation acc\"); ax1.set_title(\"Training and validation accuracy\"); ax1.legend()\n",
    "    ax2.plot(epochs, loss, \"bo\", label=\"Training loss\"); ax2.plot(epochs, val_loss, \"b\", label=\"Validation loss\"); ax2.set_title(\"Training and validation loss\"); ax2.legend()\n",
    "    plt.savefig(save_path); plt.close(fig)\n",
    "\n",
    "def evaluate_and_save_results(model, test_generator, run_dir):\n",
    "    \"\"\"Evaluates a single model, saves its individual reports, and returns its predictions.\"\"\"\n",
    "    print(f\"--- Generating individual report and saving results to: {run_dir} ---\")\n",
    "    y_pred_probs = model.predict(test_generator, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "    class_labels = list(test_generator.class_indices.keys())\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    report_path = run_dir / \"classification_report.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"Classification Report for {run_dir.parent.name} - {run_dir.name}\\n\" + \"=\" * 50 + \"\\n\" + report)\n",
    "    print(f\"Individual classification report saved to {report_path}\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix - {run_dir.parent.name} ({run_dir.name})\"); plt.ylabel(\"True Label\"); plt.xlabel(\"Predicted Label\")\n",
    "    cm_path = run_dir / \"confusion_matrix.png\"\n",
    "    plt.savefig(cm_path); plt.close()\n",
    "    print(f\"Individual confusion matrix plot saved to {cm_path}\")\n",
    "    return y_pred\n",
    "\n",
    "def create_combined_confusion_matrix(predictions_dict, y_true, class_labels, run_number, save_dir):\n",
    "    \"\"\"Creates a single plot with 2x2 confusion matrices for all models.\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 50 + f\"\\nGenerating Combined Confusion Matrix for Run {run_number}\\n\" + \"-\" * 50)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(f\"Model Comparison: Confusion Matrices (Run {run_number})\", fontsize=20)\n",
    "    for i, (model_name, y_pred) in enumerate(predictions_dict.items()):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, ax=axes[i], annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "        axes[i].set_title(model_name); axes[i].set_ylabel(\"True Label\"); axes[i].set_xlabel(\"Predicted Label\")\n",
    "    save_path = save_dir / f\"Combined_Matrix_Run_{run_number}.png\"\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(save_path); plt.close(fig)\n",
    "    print(f\"Combined confusion matrix plot saved to {save_path}\")\n",
    "\n",
    "# --- 5. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # setup_data_directories()\n",
    "    SAVED_MODELS_DIR.mkdir(exist_ok=True); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "    train_gen, valid_gen, test_gen = create_data_generators()\n",
    "    num_classes = len(train_gen.class_indices)\n",
    "    input_shape = IMG_SIZE + (3,)\n",
    "    y_true_labels = test_gen.classes\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    models_to_train = [\n",
    "        {\"name\": \"Custom_CNN\", \"builder\": create_custom_cnn, \"epochs\": 50, \"optimizer\": \"rmsprop\"},\n",
    "        {\"name\": \"InceptionV3\", \"builder\": lambda: create_transfer_model(InceptionV3, input_shape, num_classes), \"epochs\": 50, \"optimizer\": \"rmsprop\"},\n",
    "        {\"name\": \"VGG16\", \"builder\": lambda: create_transfer_model(VGG16, input_shape, num_classes), \"epochs\": 50, \"optimizer\": \"rmsprop\"},\n",
    "        {\"name\": \"ResNet50\", \"builder\": lambda: create_transfer_model(ResNet50, input_shape, num_classes), \"epochs\": 50, \"optimizer\": \"rmsprop\"},\n",
    "    ]\n",
    "\n",
    "    for run_number in range(1, NUM_TRAINING_RUNS + 1):\n",
    "        run_predictions = {}\n",
    "        print(\"\\n\" + \"#\" * 70 + f\"\\n### PROCESSING RUN {run_number}/{NUM_TRAINING_RUNS}\\n\" + \"#\" * 70)\n",
    "\n",
    "        for config in models_to_train:\n",
    "            model_name = config[\"name\"]\n",
    "            model_path = SAVED_MODELS_DIR / f\"{model_name}.keras\"\n",
    "            run_dir = RESULTS_DIR / model_name / f\"Run_{run_number}\"\n",
    "            print(\"\\n\" + \"=\" * 60 + f\"\\n=== Training {model_name} for Run {run_number} ===\\n\" + \"=\" * 60)\n",
    "            run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if model_path.exists():\n",
    "                print(f\"Loading existing model from: {model_path}\")\n",
    "                model = models.load_model(model_path)\n",
    "            else:\n",
    "                print(\"Creating a new model...\")\n",
    "                model = config[\"builder\"](input_shape, num_classes) if model_name == \"Custom_CNN\" else config[\"builder\"]()\n",
    "                model.compile(loss=\"categorical_crossentropy\", optimizer=config[\"optimizer\"], metrics=[\"acc\"])\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n",
    "            history = model.fit(train_gen, epochs=config[\"epochs\"], validation_data=valid_gen, callbacks=[early_stopping])\n",
    "            print(f\"Saving model to: {model_path}\"); model.save(model_path)\n",
    "            history_plot_path = run_dir / \"training_history.png\"\n",
    "            save_history_plot(history, f\"{model_name} - Run {run_number}\", history_plot_path)\n",
    "            print(f\"Training history plot saved to {history_plot_path}\")\n",
    "            print(\"\\n--- Performing quick test evaluation ---\")\n",
    "            eval_test = model.evaluate(test_gen, verbose=0)\n",
    "            print(f\"--> Test Accuracy: {eval_test[1] * 100:.2f}%\")\n",
    "\n",
    "            y_pred = evaluate_and_save_results(model, test_gen, run_dir)\n",
    "            run_predictions[model_name] = y_pred\n",
    "\n",
    "        create_combined_confusion_matrix(run_predictions, y_true_labels, class_names, run_number, RESULTS_DIR)\n",
    "\n",
    "    print(\"\\n\\nAll training and evaluation cycles complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myTF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
